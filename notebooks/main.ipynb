{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Big Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pré-tratamento**\n",
    "\n",
    "***Nesta secção irá ser feito o pré-tratamento dos dados***. Concretamente iremos ***fundir os dados*** provenientes dos diferentes datasets, *\"alinhando-os\"* pelas colunas *\"Entidade\"* (equivalente a nome do país), *\"Code\"* (equivalente ao código identificador IUPAC do país) e *\"Year\"* (correspondente ao ano em que foram recolhidos os dados).\n",
    "\n",
    "Iremos também *\"aparar\"* os dados em função do indicador limitante, neste caso o ano de dados disponíveis, sendo o ***período de 1990 a 2016*** o período comum a todos os datasets.\n",
    "\n",
    "**Para efeitos de comparação** iremos fazer merge dos datasets usando o package Pandas e PySpark, de modo a comparar a eficiência de ambos em diferentes contextos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-tratamento utilizando Pandas"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importamos o módulo time para efeitos de comparação de tempos de execução\n",
    "import time\n",
    "\n",
    "# Carregamento dos datasets\n",
    "import pandas as pd\n",
    "\n",
    "pd_start = time.time() # inicio do contador pandas"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset on world population\n",
    "pop = pd.read_csv('../datasets/raw/population.csv')\n",
    "pop.rename(columns={'Entity': 'Country'}, inplace = True)\n",
    "pop = pop[pop['Year'] >= 1990]\n",
    "pop = pop[pop['Year'] <= 2016]\n",
    "pop.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset on obesity\n",
    "obes = pd.read_csv('../datasets/raw/share-of-adults-defined-as-obese.csv')\n",
    "obes.rename(columns={'Entity': 'Country'}, inplace = True)\n",
    "obes = obes[obes['Year'] >= 1990]\n",
    "obes = obes[obes['Year'] <= 2016] \n",
    "obes.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset on mental disorders prevalence\n",
    "mental = pd.read_csv('../datasets/raw/mental-illnesses-prevalence.csv')\n",
    "mental.rename(columns={'Entity': 'Country'}, inplace = True)\n",
    "mental = mental[mental['Year'] >= 1990]\n",
    "mental = mental[mental['Year'] <= 2016] \n",
    "mental.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mergint the dataframes\n",
    "dataframes = [pop, obes, mental]\n",
    "\n",
    "fused = dataframes[0]\n",
    "\n",
    "for dataframe in dataframes[1:]:\n",
    "    try:\n",
    "        fused = pd.merge(\n",
    "            fused,\n",
    "            dataframe,\n",
    "            on = ['Country', 'Year', 'Code'],\n",
    "            how = 'inner'\n",
    "        )\n",
    "    except KeyError:\n",
    "        fused = pd.merge(\n",
    "            fused,\n",
    "            dataframe,\n",
    "            on = ['Country', 'Year'],\n",
    "            how = 'outer'\n",
    "        )\n",
    "\n",
    "# Exporting to CSV\n",
    "fused.to_csv('../datasets/processed/pd_processed_data.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd_end = time.time()\n",
    "pd_elapsed = pd_end - pd_start # tempo de execução do contador pandas\n",
    "print('Pandas took',pd_elapsed,'seconds to process data.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-tratamento de dados utilizando Spark"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pandas to PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "pop = spark.read.csv('../datasets/raw/population.csv', header=True)\n",
    "obes = spark.read.csv('../datasets/raw/share-of-adults-defined-as-obese.csv', header=True)\n",
    "mental = spark.read.csv('../datasets/raw/mental-illnesses-prevalence.csv', header=True)\n",
    "\n",
    "# Rename columns\n",
    "pop = pop.withColumnRenamed('Entity', 'Country')\n",
    "obes = obes.withColumnRenamed('Entity', 'Country')\n",
    "mental = mental.withColumnRenamed('Entity', 'Country')\n",
    "\n",
    "# Merge datasets\n",
    "fused = pop.join(obes, ['Country', 'Year', 'Code'], 'inner') \\\n",
    "           .join(mental, ['Country', 'Year', 'Code'], 'inner')\n",
    "\n",
    "# Drop rows with null values\n",
    "fused = fused.dropna()\n",
    "\n",
    "# Export to CSV\n",
    "fused.coalesce(1).write.option(\"header\", \"true\").csv('../datasets/processed/spark_processed_data.csv')\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Comparamos o tempo de computação para cada um dos modelos\n",
    "spark_elapsed = time.time()\n",
    "print(f'Pandas took {round(pd_elapsed, 3)} seconds.',\n",
    "      f'Spark took {round(spark_elapsed, 3)} seconds.',\n",
    "      sep = '\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs.:\n",
    "Pandas completed the pre-processing task in approximately 0.15 seconds, whereas Spark took around 6.5 seconds to accomplish the same task. This significant difference in processing time underscores the efficiency of Pandas for smaller datasets, where its lightweight nature and streamlined processes result in faster execution.\n",
    "\n",
    "However, it is essential to recognize that Spark's strength lies in its ability to handle larger-scale datasets efficiently. Despite the longer processing time observed in our experiment, Spark has demonstrated superior performance in processing datasets with millions of rows, as reported by our colleagues.\n",
    "\n",
    "Therefore, while Spark may not be the optimal choice for every pre-processing task, particularly for smaller datasets, its capabilities shine when dealing with large-scale data operations. The selection of pre-processing tools should be tailored to the specific requirements and characteristics of the dataset, ensuring optimal performance and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Loading data to database (MongoDB)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "uri = 'YOUR URL'\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('YOUR API')) # replace YOUR API with your MongoDB API\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Access database\n",
    "db = client.get_database(\"BigData\")\n",
    "\n",
    "# Access/create collection\n",
    "collection = db.get_collection(\"ObesPovMen\")\n",
    "collection\n",
    "\n",
    "# Read CSV file using pandas\n",
    "csv_file = \"../datasets/processed/pd_processed_data.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "data.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert DataFrame to dictionary\n",
    "data_dict = data.to_dict(orient='records')\n",
    "print(data_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Insert data into MongoDB collection\n",
    "collection.insert_many(data_dict)\n",
    "\n",
    "# Close connection\n",
    "client.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
